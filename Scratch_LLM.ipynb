{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UfUxbsaaCK3JN5TuL5rJgifxrD9VvwIQ",
      "authorship_tag": "ABX9TyOT3tt0X+VjdsrLC1AMe8eb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousif47/Mini-LLM/blob/main/Scratch_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch-1Z_KoG9qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e519e3-36bb-4007-d6b5-760c9ba717bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "\n",
        "print(\"Number of characters in text file :\", len(data_text.read()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mYOG98Sl0PC",
        "outputId": "b2ca9ef6-d0da-4fad-ccfd-eed7b8bbc7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters in text file : 7130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the text data\n",
        "# Basic text preprocessing because a simple dataset with minimal noise has been selected to simplify the code\n",
        "# To maintain uniformity all words has been converted to lower case and any words with length less than 3 has been removed\n",
        "# Punctuations are also removed\n",
        "\n",
        "def text_preprocess(text):\n",
        "\n",
        "    # Lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "\n",
        "    # Remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words = []\n",
        "\n",
        "    # Remove short words\n",
        "    for i in newString.split():\n",
        "        if len(i) >= 3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# Calling the preprocessor\n",
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "data_preprocessed = text_preprocess(data_text.read())\n",
        "\n",
        "print(data_preprocessed[:100])"
      ],
      "metadata": {
        "id": "cyXO-fBsmn6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bf969b-e928-4b80-8e5b-e5ceb3ec6a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "being healthy and fit simple terms means taking good care the body should remember that healthy mind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences\n",
        "# Taking 40 characters as context, model will try to predict the next character\n",
        "\n",
        "def sequencer(text):\n",
        "    length = 40\n",
        "    sequences = list()\n",
        "\n",
        "    for i in range(length, len(text)):\n",
        "\n",
        "        # Select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(seq)\n",
        "\n",
        "    print(\"Total Sequences: %d\" % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# Calling the sequencer\n",
        "sequences = sequencer(data_preprocessed)\n",
        "print(\"\\n\", sequences[0], \"\\n\", sequences[1], \"\\n\", sequences[2], \"\\n\", sequences[3], \"\\n\", sequences[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T39XEfSXmT0r",
        "outputId": "28b9dd8b-7509-49c8-eca2-6b7e36ccc922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 6322\n",
            "\n",
            " being healthy and fit simple terms means  \n",
            " eing healthy and fit simple terms means t \n",
            " ing healthy and fit simple terms means ta \n",
            " ng healthy and fit simple terms means tak \n",
            " g healthy and fit simple terms means taki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the sequences\n",
        "\n",
        "chars = sorted(list(set(data_preprocessed)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encoder(seq):\n",
        "\n",
        "    sequences = list()\n",
        "\n",
        "    for line in seq:\n",
        "\n",
        "        # Integer encode line\n",
        "        encoded = [mapping[char] for char in line]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(encoded)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Calling the encoder\n",
        "encoded_seq = encoder(sequences)\n",
        "print(encoded_seq[0], \"\\n\", encoded_seq[1], \"\\n\", encoded_seq[2], \"\\n\", encoded_seq[3], \"\\n\", encoded_seq[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLyzWf3Nqs4N",
        "outputId": "4185f904-7b28-4b64-9440-d4a04327cf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0] \n",
            " [5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20] \n",
            " [9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1] \n",
            " [14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11] \n",
            " [7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and testing split from the dataset\n",
        "\n",
        "# Vocabulary size\n",
        "vocabulary = len(mapping)\n",
        "encoded_seq = np.array(encoded_seq)\n",
        "\n",
        "X, y = encoded_seq[:,:-1], encoded_seq[:,-1]\n",
        "\n",
        "# One hot encode y\n",
        "y = to_categorical(y, num_classes = vocabulary)\n",
        "\n",
        "# Training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)\n",
        "\n",
        "print(\"Train set:\", X_train.shape, \"Test set:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNmXZjRYtdIA",
        "outputId": "9262f1c7-17f1-467e-ddd3-0dc09a8d510c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (5689, 40) Test set: (633, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the LLM\n",
        "# Embedding layer of Keras is used to creat a 50 dimension embedding for each character\n",
        "# GRU layer is used as base model with 150 timestamps\n",
        "# A Dense layer is used with a softmax activation function for prediction\n",
        "\n",
        "# Defining\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary, 50, input_length = 40, trainable = True))\n",
        "model.add(GRU(150, recurrent_dropout = 0.1, dropout = 0.1))\n",
        "model.add(Dense(vocabulary, activation = \"softmax\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Compiling\n",
        "model.compile(loss = \"categorical_crossentropy\", metrics = [\"acc\"], optimizer=\"adam\")\n",
        "\n",
        "# Fitting\n",
        "model.fit(X_train, y_train, epochs = 100, verbose = 2, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4i-ebMMuRPj",
        "outputId": "c4332bf7-c910-4e43-8f69-e4b86038e893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 40, 50)            1350      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 150)               90900     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 27)                4077      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "178/178 - 29s - loss: 2.7669 - acc: 0.2053 - val_loss: 2.5144 - val_acc: 0.2464 - 29s/epoch - 163ms/step\n",
            "Epoch 2/100\n",
            "178/178 - 28s - loss: 2.2930 - acc: 0.3173 - val_loss: 2.2999 - val_acc: 0.3160 - 28s/epoch - 158ms/step\n",
            "Epoch 3/100\n",
            "178/178 - 31s - loss: 2.1309 - acc: 0.3660 - val_loss: 2.2175 - val_acc: 0.3412 - 31s/epoch - 175ms/step\n",
            "Epoch 4/100\n",
            "178/178 - 23s - loss: 2.0055 - acc: 0.4011 - val_loss: 2.1007 - val_acc: 0.3697 - 23s/epoch - 129ms/step\n",
            "Epoch 5/100\n",
            "178/178 - 27s - loss: 1.8956 - acc: 0.4282 - val_loss: 2.0254 - val_acc: 0.3934 - 27s/epoch - 149ms/step\n",
            "Epoch 6/100\n",
            "178/178 - 21s - loss: 1.7899 - acc: 0.4575 - val_loss: 1.9619 - val_acc: 0.4202 - 21s/epoch - 120ms/step\n",
            "Epoch 7/100\n",
            "178/178 - 27s - loss: 1.6852 - acc: 0.4911 - val_loss: 1.8819 - val_acc: 0.4376 - 27s/epoch - 151ms/step\n",
            "Epoch 8/100\n",
            "178/178 - 22s - loss: 1.6000 - acc: 0.5122 - val_loss: 1.8176 - val_acc: 0.4613 - 22s/epoch - 121ms/step\n",
            "Epoch 9/100\n",
            "178/178 - 30s - loss: 1.5133 - acc: 0.5454 - val_loss: 1.8072 - val_acc: 0.4866 - 30s/epoch - 169ms/step\n",
            "Epoch 10/100\n",
            "178/178 - 24s - loss: 1.4301 - acc: 0.5739 - val_loss: 1.7506 - val_acc: 0.4976 - 24s/epoch - 137ms/step\n",
            "Epoch 11/100\n",
            "178/178 - 23s - loss: 1.3564 - acc: 0.5913 - val_loss: 1.7130 - val_acc: 0.5150 - 23s/epoch - 127ms/step\n",
            "Epoch 12/100\n",
            "178/178 - 29s - loss: 1.2867 - acc: 0.6150 - val_loss: 1.7108 - val_acc: 0.5308 - 29s/epoch - 161ms/step\n",
            "Epoch 13/100\n",
            "178/178 - 30s - loss: 1.2197 - acc: 0.6305 - val_loss: 1.6850 - val_acc: 0.5387 - 30s/epoch - 166ms/step\n",
            "Epoch 14/100\n",
            "178/178 - 24s - loss: 1.1527 - acc: 0.6500 - val_loss: 1.6691 - val_acc: 0.5229 - 24s/epoch - 136ms/step\n",
            "Epoch 15/100\n",
            "178/178 - 34s - loss: 1.1048 - acc: 0.6611 - val_loss: 1.6559 - val_acc: 0.5324 - 34s/epoch - 193ms/step\n",
            "Epoch 16/100\n",
            "178/178 - 27s - loss: 1.0465 - acc: 0.6773 - val_loss: 1.6676 - val_acc: 0.5592 - 27s/epoch - 154ms/step\n",
            "Epoch 17/100\n",
            "178/178 - 24s - loss: 0.9966 - acc: 0.6890 - val_loss: 1.6821 - val_acc: 0.5450 - 24s/epoch - 138ms/step\n",
            "Epoch 18/100\n",
            "178/178 - 23s - loss: 0.9521 - acc: 0.7114 - val_loss: 1.6661 - val_acc: 0.5640 - 23s/epoch - 129ms/step\n",
            "Epoch 19/100\n",
            "178/178 - 25s - loss: 0.9165 - acc: 0.7205 - val_loss: 1.6775 - val_acc: 0.5687 - 25s/epoch - 138ms/step\n",
            "Epoch 20/100\n",
            "178/178 - 21s - loss: 0.8824 - acc: 0.7263 - val_loss: 1.6922 - val_acc: 0.5640 - 21s/epoch - 121ms/step\n",
            "Epoch 21/100\n",
            "178/178 - 23s - loss: 0.8349 - acc: 0.7413 - val_loss: 1.7062 - val_acc: 0.5529 - 23s/epoch - 127ms/step\n",
            "Epoch 22/100\n",
            "178/178 - 23s - loss: 0.7945 - acc: 0.7564 - val_loss: 1.7307 - val_acc: 0.5592 - 23s/epoch - 128ms/step\n",
            "Epoch 23/100\n",
            "178/178 - 22s - loss: 0.7655 - acc: 0.7606 - val_loss: 1.7180 - val_acc: 0.5782 - 22s/epoch - 122ms/step\n",
            "Epoch 24/100\n",
            "178/178 - 25s - loss: 0.7334 - acc: 0.7782 - val_loss: 1.7218 - val_acc: 0.5719 - 25s/epoch - 138ms/step\n",
            "Epoch 25/100\n",
            "178/178 - 22s - loss: 0.7000 - acc: 0.7868 - val_loss: 1.7460 - val_acc: 0.5545 - 22s/epoch - 122ms/step\n",
            "Epoch 26/100\n",
            "178/178 - 24s - loss: 0.6669 - acc: 0.7935 - val_loss: 1.7965 - val_acc: 0.5624 - 24s/epoch - 137ms/step\n",
            "Epoch 27/100\n",
            "178/178 - 21s - loss: 0.6589 - acc: 0.7968 - val_loss: 1.8030 - val_acc: 0.5687 - 21s/epoch - 121ms/step\n",
            "Epoch 28/100\n",
            "178/178 - 25s - loss: 0.6247 - acc: 0.8070 - val_loss: 1.7890 - val_acc: 0.5719 - 25s/epoch - 138ms/step\n",
            "Epoch 29/100\n",
            "178/178 - 22s - loss: 0.6066 - acc: 0.8079 - val_loss: 1.8386 - val_acc: 0.5735 - 22s/epoch - 122ms/step\n",
            "Epoch 30/100\n",
            "178/178 - 24s - loss: 0.5754 - acc: 0.8246 - val_loss: 1.8767 - val_acc: 0.5640 - 24s/epoch - 136ms/step\n",
            "Epoch 31/100\n",
            "178/178 - 22s - loss: 0.5635 - acc: 0.8242 - val_loss: 1.8902 - val_acc: 0.5671 - 22s/epoch - 123ms/step\n",
            "Epoch 32/100\n",
            "178/178 - 24s - loss: 0.5491 - acc: 0.8274 - val_loss: 1.9247 - val_acc: 0.5735 - 24s/epoch - 138ms/step\n",
            "Epoch 33/100\n",
            "178/178 - 22s - loss: 0.5352 - acc: 0.8314 - val_loss: 1.9481 - val_acc: 0.5592 - 22s/epoch - 122ms/step\n",
            "Epoch 34/100\n",
            "178/178 - 24s - loss: 0.5131 - acc: 0.8402 - val_loss: 1.9790 - val_acc: 0.5513 - 24s/epoch - 137ms/step\n",
            "Epoch 35/100\n",
            "178/178 - 22s - loss: 0.5001 - acc: 0.8457 - val_loss: 1.9653 - val_acc: 0.5750 - 22s/epoch - 121ms/step\n",
            "Epoch 36/100\n",
            "178/178 - 23s - loss: 0.4786 - acc: 0.8543 - val_loss: 2.0061 - val_acc: 0.5640 - 23s/epoch - 132ms/step\n",
            "Epoch 37/100\n",
            "178/178 - 22s - loss: 0.4580 - acc: 0.8634 - val_loss: 2.0366 - val_acc: 0.5577 - 22s/epoch - 126ms/step\n",
            "Epoch 38/100\n",
            "178/178 - 23s - loss: 0.4480 - acc: 0.8669 - val_loss: 2.0497 - val_acc: 0.5624 - 23s/epoch - 130ms/step\n",
            "Epoch 39/100\n",
            "178/178 - 23s - loss: 0.4387 - acc: 0.8629 - val_loss: 2.0762 - val_acc: 0.5608 - 23s/epoch - 129ms/step\n",
            "Epoch 40/100\n",
            "178/178 - 22s - loss: 0.4329 - acc: 0.8638 - val_loss: 2.1010 - val_acc: 0.5592 - 22s/epoch - 126ms/step\n",
            "Epoch 41/100\n",
            "178/178 - 24s - loss: 0.4216 - acc: 0.8687 - val_loss: 2.1302 - val_acc: 0.5513 - 24s/epoch - 135ms/step\n",
            "Epoch 42/100\n",
            "178/178 - 22s - loss: 0.4155 - acc: 0.8708 - val_loss: 2.1233 - val_acc: 0.5750 - 22s/epoch - 123ms/step\n",
            "Epoch 43/100\n",
            "178/178 - 24s - loss: 0.3980 - acc: 0.8741 - val_loss: 2.1549 - val_acc: 0.5640 - 24s/epoch - 136ms/step\n",
            "Epoch 44/100\n",
            "178/178 - 22s - loss: 0.4034 - acc: 0.8692 - val_loss: 2.1603 - val_acc: 0.5656 - 22s/epoch - 124ms/step\n",
            "Epoch 45/100\n",
            "178/178 - 25s - loss: 0.3835 - acc: 0.8803 - val_loss: 2.1941 - val_acc: 0.5671 - 25s/epoch - 139ms/step\n",
            "Epoch 46/100\n",
            "178/178 - 22s - loss: 0.3675 - acc: 0.8868 - val_loss: 2.1977 - val_acc: 0.5656 - 22s/epoch - 125ms/step\n",
            "Epoch 47/100\n",
            "178/178 - 25s - loss: 0.3793 - acc: 0.8806 - val_loss: 2.2159 - val_acc: 0.5561 - 25s/epoch - 139ms/step\n",
            "Epoch 48/100\n",
            "178/178 - 22s - loss: 0.3630 - acc: 0.8875 - val_loss: 2.2230 - val_acc: 0.5577 - 22s/epoch - 124ms/step\n",
            "Epoch 49/100\n",
            "178/178 - 25s - loss: 0.3499 - acc: 0.8894 - val_loss: 2.2457 - val_acc: 0.5608 - 25s/epoch - 139ms/step\n",
            "Epoch 50/100\n",
            "178/178 - 22s - loss: 0.3521 - acc: 0.8887 - val_loss: 2.2800 - val_acc: 0.5592 - 22s/epoch - 122ms/step\n",
            "Epoch 51/100\n",
            "178/178 - 24s - loss: 0.3394 - acc: 0.8922 - val_loss: 2.3141 - val_acc: 0.5545 - 24s/epoch - 137ms/step\n",
            "Epoch 52/100\n",
            "178/178 - 22s - loss: 0.3321 - acc: 0.8966 - val_loss: 2.3422 - val_acc: 0.5624 - 22s/epoch - 122ms/step\n",
            "Epoch 53/100\n",
            "178/178 - 25s - loss: 0.3344 - acc: 0.8938 - val_loss: 2.4010 - val_acc: 0.5529 - 25s/epoch - 138ms/step\n",
            "Epoch 54/100\n",
            "178/178 - 22s - loss: 0.3338 - acc: 0.8940 - val_loss: 2.3552 - val_acc: 0.5592 - 22s/epoch - 125ms/step\n",
            "Epoch 55/100\n",
            "178/178 - 25s - loss: 0.3135 - acc: 0.9024 - val_loss: 2.3721 - val_acc: 0.5735 - 25s/epoch - 138ms/step\n",
            "Epoch 56/100\n",
            "178/178 - 22s - loss: 0.3212 - acc: 0.8984 - val_loss: 2.3839 - val_acc: 0.5671 - 22s/epoch - 121ms/step\n",
            "Epoch 57/100\n",
            "178/178 - 24s - loss: 0.2977 - acc: 0.9054 - val_loss: 2.4216 - val_acc: 0.5624 - 24s/epoch - 137ms/step\n",
            "Epoch 58/100\n",
            "178/178 - 22s - loss: 0.3176 - acc: 0.9000 - val_loss: 2.4505 - val_acc: 0.5640 - 22s/epoch - 122ms/step\n",
            "Epoch 59/100\n",
            "178/178 - 24s - loss: 0.3043 - acc: 0.8996 - val_loss: 2.4517 - val_acc: 0.5561 - 24s/epoch - 136ms/step\n",
            "Epoch 60/100\n",
            "178/178 - 22s - loss: 0.2986 - acc: 0.9044 - val_loss: 2.4546 - val_acc: 0.5592 - 22s/epoch - 123ms/step\n",
            "Epoch 61/100\n",
            "178/178 - 24s - loss: 0.2875 - acc: 0.9082 - val_loss: 2.4874 - val_acc: 0.5608 - 24s/epoch - 133ms/step\n",
            "Epoch 62/100\n",
            "178/178 - 22s - loss: 0.2832 - acc: 0.9126 - val_loss: 2.5139 - val_acc: 0.5592 - 22s/epoch - 125ms/step\n",
            "Epoch 63/100\n",
            "178/178 - 23s - loss: 0.2876 - acc: 0.9065 - val_loss: 2.4905 - val_acc: 0.5624 - 23s/epoch - 128ms/step\n",
            "Epoch 64/100\n",
            "178/178 - 23s - loss: 0.2794 - acc: 0.9109 - val_loss: 2.5591 - val_acc: 0.5592 - 23s/epoch - 129ms/step\n",
            "Epoch 65/100\n",
            "178/178 - 22s - loss: 0.2794 - acc: 0.9104 - val_loss: 2.5588 - val_acc: 0.5735 - 22s/epoch - 123ms/step\n",
            "Epoch 66/100\n",
            "178/178 - 24s - loss: 0.2733 - acc: 0.9133 - val_loss: 2.5577 - val_acc: 0.5814 - 24s/epoch - 136ms/step\n",
            "Epoch 67/100\n",
            "178/178 - 22s - loss: 0.2851 - acc: 0.9105 - val_loss: 2.6102 - val_acc: 0.5592 - 22s/epoch - 123ms/step\n",
            "Epoch 68/100\n",
            "178/178 - 24s - loss: 0.2581 - acc: 0.9216 - val_loss: 2.6337 - val_acc: 0.5577 - 24s/epoch - 136ms/step\n",
            "Epoch 69/100\n",
            "178/178 - 22s - loss: 0.2680 - acc: 0.9149 - val_loss: 2.6738 - val_acc: 0.5592 - 22s/epoch - 123ms/step\n",
            "Epoch 70/100\n",
            "178/178 - 24s - loss: 0.2681 - acc: 0.9137 - val_loss: 2.6238 - val_acc: 0.5561 - 24s/epoch - 137ms/step\n",
            "Epoch 71/100\n",
            "178/178 - 22s - loss: 0.2644 - acc: 0.9153 - val_loss: 2.6493 - val_acc: 0.5419 - 22s/epoch - 121ms/step\n",
            "Epoch 72/100\n",
            "178/178 - 25s - loss: 0.2551 - acc: 0.9184 - val_loss: 2.7154 - val_acc: 0.5513 - 25s/epoch - 138ms/step\n",
            "Epoch 73/100\n",
            "178/178 - 22s - loss: 0.2523 - acc: 0.9204 - val_loss: 2.6804 - val_acc: 0.5608 - 22s/epoch - 122ms/step\n",
            "Epoch 74/100\n",
            "178/178 - 24s - loss: 0.2481 - acc: 0.9213 - val_loss: 2.7314 - val_acc: 0.5608 - 24s/epoch - 136ms/step\n",
            "Epoch 75/100\n",
            "178/178 - 21s - loss: 0.2491 - acc: 0.9209 - val_loss: 2.7329 - val_acc: 0.5466 - 21s/epoch - 120ms/step\n",
            "Epoch 76/100\n",
            "178/178 - 24s - loss: 0.2494 - acc: 0.9251 - val_loss: 2.7444 - val_acc: 0.5545 - 24s/epoch - 133ms/step\n",
            "Epoch 77/100\n",
            "178/178 - 23s - loss: 0.2485 - acc: 0.9172 - val_loss: 2.7647 - val_acc: 0.5482 - 23s/epoch - 127ms/step\n",
            "Epoch 78/100\n",
            "178/178 - 24s - loss: 0.2433 - acc: 0.9174 - val_loss: 2.7698 - val_acc: 0.5450 - 24s/epoch - 132ms/step\n",
            "Epoch 79/100\n",
            "178/178 - 23s - loss: 0.2388 - acc: 0.9241 - val_loss: 2.7599 - val_acc: 0.5577 - 23s/epoch - 127ms/step\n",
            "Epoch 80/100\n",
            "178/178 - 23s - loss: 0.2443 - acc: 0.9191 - val_loss: 2.7820 - val_acc: 0.5545 - 23s/epoch - 126ms/step\n",
            "Epoch 81/100\n",
            "178/178 - 24s - loss: 0.2321 - acc: 0.9272 - val_loss: 2.8531 - val_acc: 0.5450 - 24s/epoch - 134ms/step\n",
            "Epoch 82/100\n",
            "178/178 - 22s - loss: 0.2459 - acc: 0.9198 - val_loss: 2.8560 - val_acc: 0.5466 - 22s/epoch - 125ms/step\n",
            "Epoch 83/100\n",
            "178/178 - 24s - loss: 0.2463 - acc: 0.9163 - val_loss: 2.7981 - val_acc: 0.5529 - 24s/epoch - 136ms/step\n",
            "Epoch 84/100\n",
            "178/178 - 22s - loss: 0.2355 - acc: 0.9242 - val_loss: 2.8455 - val_acc: 0.5577 - 22s/epoch - 124ms/step\n",
            "Epoch 85/100\n",
            "178/178 - 24s - loss: 0.2306 - acc: 0.9267 - val_loss: 2.8519 - val_acc: 0.5434 - 24s/epoch - 135ms/step\n",
            "Epoch 86/100\n",
            "178/178 - 22s - loss: 0.2223 - acc: 0.9304 - val_loss: 2.8502 - val_acc: 0.5577 - 22s/epoch - 122ms/step\n",
            "Epoch 87/100\n",
            "178/178 - 25s - loss: 0.2305 - acc: 0.9216 - val_loss: 2.8280 - val_acc: 0.5513 - 25s/epoch - 139ms/step\n",
            "Epoch 88/100\n",
            "178/178 - 22s - loss: 0.2310 - acc: 0.9244 - val_loss: 2.8629 - val_acc: 0.5419 - 22s/epoch - 123ms/step\n",
            "Epoch 89/100\n",
            "178/178 - 25s - loss: 0.2285 - acc: 0.9267 - val_loss: 2.8672 - val_acc: 0.5513 - 25s/epoch - 139ms/step\n",
            "Epoch 90/100\n",
            "178/178 - 22s - loss: 0.2295 - acc: 0.9267 - val_loss: 2.9246 - val_acc: 0.5482 - 22s/epoch - 125ms/step\n",
            "Epoch 91/100\n",
            "178/178 - 25s - loss: 0.2280 - acc: 0.9244 - val_loss: 2.8946 - val_acc: 0.5671 - 25s/epoch - 140ms/step\n",
            "Epoch 92/100\n",
            "178/178 - 22s - loss: 0.2331 - acc: 0.9225 - val_loss: 2.8864 - val_acc: 0.5577 - 22s/epoch - 125ms/step\n",
            "Epoch 93/100\n",
            "178/178 - 25s - loss: 0.2187 - acc: 0.9290 - val_loss: 2.8988 - val_acc: 0.5561 - 25s/epoch - 139ms/step\n",
            "Epoch 94/100\n",
            "178/178 - 22s - loss: 0.2150 - acc: 0.9304 - val_loss: 2.8797 - val_acc: 0.5577 - 22s/epoch - 125ms/step\n",
            "Epoch 95/100\n",
            "178/178 - 25s - loss: 0.2141 - acc: 0.9278 - val_loss: 2.9037 - val_acc: 0.5529 - 25s/epoch - 140ms/step\n",
            "Epoch 96/100\n",
            "178/178 - 22s - loss: 0.2099 - acc: 0.9332 - val_loss: 2.9343 - val_acc: 0.5450 - 22s/epoch - 124ms/step\n",
            "Epoch 97/100\n",
            "178/178 - 24s - loss: 0.2180 - acc: 0.9290 - val_loss: 2.9905 - val_acc: 0.5529 - 24s/epoch - 138ms/step\n",
            "Epoch 98/100\n",
            "178/178 - 22s - loss: 0.2174 - acc: 0.9263 - val_loss: 3.0154 - val_acc: 0.5482 - 22s/epoch - 121ms/step\n",
            "Epoch 99/100\n",
            "178/178 - 24s - loss: 0.2042 - acc: 0.9318 - val_loss: 2.9656 - val_acc: 0.5624 - 24s/epoch - 137ms/step\n",
            "Epoch 100/100\n",
            "178/178 - 22s - loss: 0.2231 - acc: 0.9251 - val_loss: 2.9970 - val_acc: 0.5545 - 22s/epoch - 121ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x787987737010>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a sequence of characters with the LLM\n",
        "# The LLM will attempt to predict the next 20 characters after taking bla bla\n",
        "\n",
        "def generator(model, mapping, seq_length, seed_text, n_chars):\n",
        "\n",
        "\tinput_text = seed_text\n",
        "\n",
        "\t# Generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\n",
        "\t\t# Encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in input_text]\n",
        "\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen = seq_length, truncating = \"pre\")\n",
        "\n",
        "\t\t# Predict character\n",
        "\t\tyhat = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "\t\t# Reverse map integer to character\n",
        "\t\tout_char = \"\"\n",
        "\n",
        "\t\tfor char, index in mapping.items():\n",
        "\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# Append\n",
        "\t\tinput_text += char\n",
        "\n",
        "\treturn input_text\n",
        "\n",
        "# Calling the generator\n",
        "input = \"\"\n",
        "print(len(input))\n",
        "print(generator(model, mapping, 40, input.lower(), 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "p5yLmwy6vyP-",
        "outputId": "e054209e-c9d7-4249-cbf4-1dd46129cb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "20/20 [==============================] - 1s 18ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-d1f82f9e6842>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-94-d1f82f9e6842>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(model, mapping, seq_length, seed_text, n_chars)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                                 \u001b[0mout_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    }
  ]
}