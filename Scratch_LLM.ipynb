{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UfUxbsaaCK3JN5TuL5rJgifxrD9VvwIQ",
      "authorship_tag": "ABX9TyPru2FQroL+XdoSiaAZvODz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousif47/Mini-LLM/blob/main/Scratch_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ch-1Z_KoG9qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fa349f-fec0-4ede-ee63-9ef93100abf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "\n",
        "print(data_text.read(100), \"\\n\\nNumber of characters in text file :\", len(data_text.read()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mYOG98Sl0PC",
        "outputId": "8b0bc5bb-47ba-46ab-ca10-e6875743d208"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being healthy and fit in simple terms means taking good care of the body. We should remember that a  \n",
            "\n",
            "Number of characters in text file : 7030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the text data\n",
        "# Basic text preprocessing because a simple dataset with minimal noise has been selected to simplify the code\n",
        "# To maintain uniformity all words has been converted to lower case and any words with length less than 3 has been removed\n",
        "# Punctuations are also removed\n",
        "\n",
        "def text_preprocess(text):\n",
        "\n",
        "    # Lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "\n",
        "    # Remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words = []\n",
        "\n",
        "    # Remove short words\n",
        "    for i in newString.split():\n",
        "        if len(i) >= 3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# Calling the preprocessor\n",
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "data_preprocessed = text_preprocess(data_text.read())\n",
        "\n",
        "print(data_preprocessed[:100])"
      ],
      "metadata": {
        "id": "cyXO-fBsmn6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece73b06-e467-41f4-cd14-cc55f18aab39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "being healthy and fit simple terms means taking good care the body should remember that healthy mind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences\n",
        "# Taking 40 characters as context, model will try to predict the next character\n",
        "\n",
        "def sequencer(text):\n",
        "    length = 40\n",
        "    sequences = list()\n",
        "\n",
        "    for i in range(length, len(text)):\n",
        "\n",
        "        # Select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(seq)\n",
        "\n",
        "    print(\"Total Sequences: %d\" % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# Calling the sequencer\n",
        "sequences = sequencer(data_preprocessed)\n",
        "print(\"\\n\", sequences[0], \"\\n\", sequences[1], \"\\n\", sequences[2], \"\\n\", sequences[3], \"\\n\", sequences[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T39XEfSXmT0r",
        "outputId": "273a914d-bda9-4348-fe03-a800980bb43d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 6322\n",
            "\n",
            " being healthy and fit simple terms means  \n",
            " eing healthy and fit simple terms means t \n",
            " ing healthy and fit simple terms means ta \n",
            " ng healthy and fit simple terms means tak \n",
            " g healthy and fit simple terms means taki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the sequences\n",
        "\n",
        "chars = sorted(list(set(data_preprocessed)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encoder(seq):\n",
        "\n",
        "    sequences = list()\n",
        "\n",
        "    for line in seq:\n",
        "\n",
        "        # Integer encode line\n",
        "        encoded = [mapping[char] for char in line]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(encoded)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Calling the encoder\n",
        "encoded_seq = encoder(sequences)\n",
        "print(encoded_seq[0], \"\\n\", encoded_seq[1], \"\\n\", encoded_seq[2], \"\\n\", encoded_seq[3], \"\\n\", encoded_seq[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLyzWf3Nqs4N",
        "outputId": "f3fec997-2a58-4334-e0e9-b3b83e054758"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0] \n",
            " [5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20] \n",
            " [9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1] \n",
            " [14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11] \n",
            " [7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and testing split from the dataset\n",
        "\n",
        "# Vocabulary size\n",
        "vocabulary = len(mapping)\n",
        "encoded_seq = np.array(encoded_seq)\n",
        "\n",
        "X, y = encoded_seq[:,:-1], encoded_seq[:,-1]\n",
        "\n",
        "# One hot encode y\n",
        "y = to_categorical(y, num_classes = vocabulary)\n",
        "\n",
        "# Training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 47)\n",
        "\n",
        "print(\"Train set:\", X_train.shape, \"Test set:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNmXZjRYtdIA",
        "outputId": "407d0b2f-07e8-445a-ee7e-b3b7e8351f90"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (5057, 40) Test set: (1265, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the LLM\n",
        "# Embedding layer of Keras is used to creat a 50 dimension embedding for each character\n",
        "# GRU layer is used as base model with 150 timestamps\n",
        "# A Dense layer is used with a softmax activation function for prediction\n",
        "\n",
        "# Defining\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary, 50, input_length = 40, trainable = True))\n",
        "model.add(GRU(150, recurrent_dropout = 0.1, dropout = 0.1))\n",
        "model.add(Dense(vocabulary, activation = \"softmax\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Compiling\n",
        "model.compile(loss = \"categorical_crossentropy\", metrics = [\"acc\"], optimizer=\"adam\")\n",
        "\n",
        "# Fitting\n",
        "model.fit(X_train, y_train, epochs = 100, verbose = 2, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4i-ebMMuRPj",
        "outputId": "43245028-7951-4a27-d845-931c31a69680"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 50)            1350      \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 150)               90900     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 27)                4077      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "159/159 - 31s - loss: 2.8417 - acc: 0.1859 - val_loss: 2.5719 - val_acc: 0.2482 - 31s/epoch - 195ms/step\n",
            "Epoch 2/100\n",
            "159/159 - 25s - loss: 2.3649 - acc: 0.2994 - val_loss: 2.2952 - val_acc: 0.2941 - 25s/epoch - 160ms/step\n",
            "Epoch 3/100\n",
            "159/159 - 23s - loss: 2.1723 - acc: 0.3548 - val_loss: 2.1964 - val_acc: 0.3439 - 23s/epoch - 144ms/step\n",
            "Epoch 4/100\n",
            "159/159 - 26s - loss: 2.0648 - acc: 0.3844 - val_loss: 2.1262 - val_acc: 0.3684 - 26s/epoch - 163ms/step\n",
            "Epoch 5/100\n",
            "159/159 - 23s - loss: 1.9676 - acc: 0.4036 - val_loss: 2.0714 - val_acc: 0.3779 - 23s/epoch - 146ms/step\n",
            "Epoch 6/100\n",
            "159/159 - 26s - loss: 1.8813 - acc: 0.4342 - val_loss: 1.9868 - val_acc: 0.4253 - 26s/epoch - 165ms/step\n",
            "Epoch 7/100\n",
            "159/159 - 26s - loss: 1.7932 - acc: 0.4619 - val_loss: 1.9259 - val_acc: 0.4245 - 26s/epoch - 164ms/step\n",
            "Epoch 8/100\n",
            "159/159 - 24s - loss: 1.7015 - acc: 0.4835 - val_loss: 1.8739 - val_acc: 0.4585 - 24s/epoch - 149ms/step\n",
            "Epoch 9/100\n",
            "159/159 - 26s - loss: 1.6238 - acc: 0.5017 - val_loss: 1.8418 - val_acc: 0.4711 - 26s/epoch - 164ms/step\n",
            "Epoch 10/100\n",
            "159/159 - 24s - loss: 1.5546 - acc: 0.5327 - val_loss: 1.7969 - val_acc: 0.4846 - 24s/epoch - 150ms/step\n",
            "Epoch 11/100\n",
            "159/159 - 27s - loss: 1.4735 - acc: 0.5612 - val_loss: 1.7822 - val_acc: 0.4893 - 27s/epoch - 167ms/step\n",
            "Epoch 12/100\n",
            "159/159 - 26s - loss: 1.4072 - acc: 0.5750 - val_loss: 1.7371 - val_acc: 0.5257 - 26s/epoch - 166ms/step\n",
            "Epoch 13/100\n",
            "159/159 - 24s - loss: 1.3429 - acc: 0.5895 - val_loss: 1.7035 - val_acc: 0.5312 - 24s/epoch - 148ms/step\n",
            "Epoch 14/100\n",
            "159/159 - 27s - loss: 1.2892 - acc: 0.6059 - val_loss: 1.6855 - val_acc: 0.5368 - 27s/epoch - 167ms/step\n",
            "Epoch 15/100\n",
            "159/159 - 26s - loss: 1.2333 - acc: 0.6280 - val_loss: 1.6869 - val_acc: 0.5368 - 26s/epoch - 161ms/step\n",
            "Epoch 16/100\n",
            "159/159 - 23s - loss: 1.1696 - acc: 0.6460 - val_loss: 1.6751 - val_acc: 0.5542 - 23s/epoch - 147ms/step\n",
            "Epoch 17/100\n",
            "159/159 - 26s - loss: 1.1142 - acc: 0.6599 - val_loss: 1.6807 - val_acc: 0.5423 - 26s/epoch - 163ms/step\n",
            "Epoch 18/100\n",
            "159/159 - 24s - loss: 1.0706 - acc: 0.6706 - val_loss: 1.6694 - val_acc: 0.5470 - 24s/epoch - 149ms/step\n",
            "Epoch 19/100\n",
            "159/159 - 26s - loss: 1.0266 - acc: 0.6850 - val_loss: 1.6638 - val_acc: 0.5549 - 26s/epoch - 162ms/step\n",
            "Epoch 20/100\n",
            "159/159 - 24s - loss: 0.9731 - acc: 0.7058 - val_loss: 1.6633 - val_acc: 0.5557 - 24s/epoch - 151ms/step\n",
            "Epoch 21/100\n",
            "159/159 - 25s - loss: 0.9469 - acc: 0.7089 - val_loss: 1.6779 - val_acc: 0.5542 - 25s/epoch - 156ms/step\n",
            "Epoch 22/100\n",
            "159/159 - 27s - loss: 0.9005 - acc: 0.7222 - val_loss: 1.6824 - val_acc: 0.5534 - 27s/epoch - 167ms/step\n",
            "Epoch 23/100\n",
            "159/159 - 23s - loss: 0.8634 - acc: 0.7301 - val_loss: 1.7029 - val_acc: 0.5557 - 23s/epoch - 145ms/step\n",
            "Epoch 24/100\n",
            "159/159 - 27s - loss: 0.8408 - acc: 0.7408 - val_loss: 1.6999 - val_acc: 0.5613 - 27s/epoch - 169ms/step\n",
            "Epoch 25/100\n",
            "159/159 - 27s - loss: 0.7893 - acc: 0.7568 - val_loss: 1.7175 - val_acc: 0.5723 - 27s/epoch - 167ms/step\n",
            "Epoch 26/100\n",
            "159/159 - 23s - loss: 0.7549 - acc: 0.7655 - val_loss: 1.7304 - val_acc: 0.5597 - 23s/epoch - 145ms/step\n",
            "Epoch 27/100\n",
            "159/159 - 27s - loss: 0.7573 - acc: 0.7637 - val_loss: 1.7472 - val_acc: 0.5605 - 27s/epoch - 167ms/step\n",
            "Epoch 28/100\n",
            "159/159 - 25s - loss: 0.6979 - acc: 0.7847 - val_loss: 1.7447 - val_acc: 0.5621 - 25s/epoch - 158ms/step\n",
            "Epoch 29/100\n",
            "159/159 - 24s - loss: 0.6924 - acc: 0.7829 - val_loss: 1.7681 - val_acc: 0.5700 - 24s/epoch - 153ms/step\n",
            "Epoch 30/100\n",
            "159/159 - 26s - loss: 0.6468 - acc: 0.7981 - val_loss: 1.7689 - val_acc: 0.5668 - 26s/epoch - 166ms/step\n",
            "Epoch 31/100\n",
            "159/159 - 23s - loss: 0.6445 - acc: 0.7999 - val_loss: 1.7933 - val_acc: 0.5518 - 23s/epoch - 143ms/step\n",
            "Epoch 32/100\n",
            "159/159 - 25s - loss: 0.6053 - acc: 0.8092 - val_loss: 1.8179 - val_acc: 0.5652 - 25s/epoch - 158ms/step\n",
            "Epoch 33/100\n",
            "159/159 - 24s - loss: 0.5852 - acc: 0.8135 - val_loss: 1.8271 - val_acc: 0.5731 - 24s/epoch - 151ms/step\n",
            "Epoch 34/100\n",
            "159/159 - 26s - loss: 0.5771 - acc: 0.8222 - val_loss: 1.8489 - val_acc: 0.5660 - 26s/epoch - 165ms/step\n",
            "Epoch 35/100\n",
            "159/159 - 25s - loss: 0.5529 - acc: 0.8226 - val_loss: 1.8675 - val_acc: 0.5723 - 25s/epoch - 160ms/step\n",
            "Epoch 36/100\n",
            "159/159 - 24s - loss: 0.5437 - acc: 0.8301 - val_loss: 1.8633 - val_acc: 0.5700 - 24s/epoch - 148ms/step\n",
            "Epoch 37/100\n",
            "159/159 - 26s - loss: 0.5294 - acc: 0.8335 - val_loss: 1.8887 - val_acc: 0.5779 - 26s/epoch - 166ms/step\n",
            "Epoch 38/100\n",
            "159/159 - 23s - loss: 0.5280 - acc: 0.8390 - val_loss: 1.9364 - val_acc: 0.5605 - 23s/epoch - 146ms/step\n",
            "Epoch 39/100\n",
            "159/159 - 26s - loss: 0.4848 - acc: 0.8543 - val_loss: 1.9478 - val_acc: 0.5676 - 26s/epoch - 163ms/step\n",
            "Epoch 40/100\n",
            "159/159 - 25s - loss: 0.4686 - acc: 0.8566 - val_loss: 1.9431 - val_acc: 0.5715 - 25s/epoch - 157ms/step\n",
            "Epoch 41/100\n",
            "159/159 - 23s - loss: 0.4781 - acc: 0.8495 - val_loss: 1.9614 - val_acc: 0.5692 - 23s/epoch - 145ms/step\n",
            "Epoch 42/100\n",
            "159/159 - 26s - loss: 0.4439 - acc: 0.8640 - val_loss: 1.9838 - val_acc: 0.5692 - 26s/epoch - 160ms/step\n",
            "Epoch 43/100\n",
            "159/159 - 23s - loss: 0.4428 - acc: 0.8651 - val_loss: 2.0082 - val_acc: 0.5731 - 23s/epoch - 144ms/step\n",
            "Epoch 44/100\n",
            "159/159 - 27s - loss: 0.4255 - acc: 0.8709 - val_loss: 2.0260 - val_acc: 0.5581 - 27s/epoch - 167ms/step\n",
            "Epoch 45/100\n",
            "159/159 - 26s - loss: 0.4066 - acc: 0.8738 - val_loss: 2.0615 - val_acc: 0.5621 - 26s/epoch - 163ms/step\n",
            "Epoch 46/100\n",
            "159/159 - 24s - loss: 0.4118 - acc: 0.8689 - val_loss: 2.0663 - val_acc: 0.5565 - 24s/epoch - 148ms/step\n",
            "Epoch 47/100\n",
            "159/159 - 26s - loss: 0.3880 - acc: 0.8806 - val_loss: 2.0936 - val_acc: 0.5652 - 26s/epoch - 166ms/step\n",
            "Epoch 48/100\n",
            "159/159 - 24s - loss: 0.3782 - acc: 0.8865 - val_loss: 2.1291 - val_acc: 0.5597 - 24s/epoch - 150ms/step\n",
            "Epoch 49/100\n",
            "159/159 - 26s - loss: 0.3841 - acc: 0.8839 - val_loss: 2.1169 - val_acc: 0.5628 - 26s/epoch - 163ms/step\n",
            "Epoch 50/100\n",
            "159/159 - 25s - loss: 0.3679 - acc: 0.8893 - val_loss: 2.1221 - val_acc: 0.5684 - 25s/epoch - 158ms/step\n",
            "Epoch 51/100\n",
            "159/159 - 23s - loss: 0.3569 - acc: 0.8924 - val_loss: 2.1798 - val_acc: 0.5605 - 23s/epoch - 142ms/step\n",
            "Epoch 52/100\n",
            "159/159 - 26s - loss: 0.3588 - acc: 0.8918 - val_loss: 2.1764 - val_acc: 0.5644 - 26s/epoch - 165ms/step\n",
            "Epoch 53/100\n",
            "159/159 - 23s - loss: 0.3400 - acc: 0.8926 - val_loss: 2.2068 - val_acc: 0.5660 - 23s/epoch - 146ms/step\n",
            "Epoch 54/100\n",
            "159/159 - 27s - loss: 0.3356 - acc: 0.8950 - val_loss: 2.2128 - val_acc: 0.5668 - 27s/epoch - 167ms/step\n",
            "Epoch 55/100\n",
            "159/159 - 23s - loss: 0.3831 - acc: 0.8746 - val_loss: 2.2412 - val_acc: 0.5589 - 23s/epoch - 143ms/step\n",
            "Epoch 56/100\n",
            "159/159 - 25s - loss: 0.3402 - acc: 0.8974 - val_loss: 2.2336 - val_acc: 0.5692 - 25s/epoch - 160ms/step\n",
            "Epoch 57/100\n",
            "159/159 - 24s - loss: 0.3237 - acc: 0.9001 - val_loss: 2.2805 - val_acc: 0.5621 - 24s/epoch - 153ms/step\n",
            "Epoch 58/100\n",
            "159/159 - 24s - loss: 0.3307 - acc: 0.8962 - val_loss: 2.3043 - val_acc: 0.5549 - 24s/epoch - 149ms/step\n",
            "Epoch 59/100\n",
            "159/159 - 27s - loss: 0.3125 - acc: 0.9009 - val_loss: 2.3262 - val_acc: 0.5621 - 27s/epoch - 167ms/step\n",
            "Epoch 60/100\n",
            "159/159 - 22s - loss: 0.2897 - acc: 0.9152 - val_loss: 2.3325 - val_acc: 0.5597 - 22s/epoch - 141ms/step\n",
            "Epoch 61/100\n",
            "159/159 - 27s - loss: 0.2976 - acc: 0.9065 - val_loss: 2.3390 - val_acc: 0.5652 - 27s/epoch - 169ms/step\n",
            "Epoch 62/100\n",
            "159/159 - 25s - loss: 0.2945 - acc: 0.9092 - val_loss: 2.3617 - val_acc: 0.5636 - 25s/epoch - 155ms/step\n",
            "Epoch 63/100\n",
            "159/159 - 23s - loss: 0.2889 - acc: 0.9120 - val_loss: 2.3709 - val_acc: 0.5613 - 23s/epoch - 148ms/step\n",
            "Epoch 64/100\n",
            "159/159 - 25s - loss: 0.2882 - acc: 0.9084 - val_loss: 2.3744 - val_acc: 0.5660 - 25s/epoch - 157ms/step\n",
            "Epoch 65/100\n",
            "159/159 - 23s - loss: 0.2810 - acc: 0.9086 - val_loss: 2.3807 - val_acc: 0.5589 - 23s/epoch - 144ms/step\n",
            "Epoch 66/100\n",
            "159/159 - 26s - loss: 0.2784 - acc: 0.9108 - val_loss: 2.4116 - val_acc: 0.5621 - 26s/epoch - 164ms/step\n",
            "Epoch 67/100\n",
            "159/159 - 23s - loss: 0.3116 - acc: 0.9063 - val_loss: 2.4335 - val_acc: 0.5621 - 23s/epoch - 144ms/step\n",
            "Epoch 68/100\n",
            "159/159 - 26s - loss: 0.2914 - acc: 0.9071 - val_loss: 2.4465 - val_acc: 0.5605 - 26s/epoch - 161ms/step\n",
            "Epoch 69/100\n",
            "159/159 - 23s - loss: 0.2716 - acc: 0.9136 - val_loss: 2.4602 - val_acc: 0.5573 - 23s/epoch - 147ms/step\n",
            "Epoch 70/100\n",
            "159/159 - 26s - loss: 0.2607 - acc: 0.9171 - val_loss: 2.4775 - val_acc: 0.5534 - 26s/epoch - 162ms/step\n",
            "Epoch 71/100\n",
            "159/159 - 26s - loss: 0.2618 - acc: 0.9171 - val_loss: 2.4892 - val_acc: 0.5565 - 26s/epoch - 163ms/step\n",
            "Epoch 72/100\n",
            "159/159 - 23s - loss: 0.2490 - acc: 0.9270 - val_loss: 2.4997 - val_acc: 0.5589 - 23s/epoch - 148ms/step\n",
            "Epoch 73/100\n",
            "159/159 - 26s - loss: 0.2483 - acc: 0.9215 - val_loss: 2.4986 - val_acc: 0.5573 - 26s/epoch - 164ms/step\n",
            "Epoch 74/100\n",
            "159/159 - 25s - loss: 0.2831 - acc: 0.9084 - val_loss: 2.4999 - val_acc: 0.5573 - 25s/epoch - 156ms/step\n",
            "Epoch 75/100\n",
            "159/159 - 24s - loss: 0.2552 - acc: 0.9219 - val_loss: 2.5279 - val_acc: 0.5605 - 24s/epoch - 153ms/step\n",
            "Epoch 76/100\n",
            "159/159 - 26s - loss: 0.2475 - acc: 0.9221 - val_loss: 2.5503 - val_acc: 0.5565 - 26s/epoch - 164ms/step\n",
            "Epoch 77/100\n",
            "159/159 - 23s - loss: 0.2417 - acc: 0.9213 - val_loss: 2.5516 - val_acc: 0.5636 - 23s/epoch - 147ms/step\n",
            "Epoch 78/100\n",
            "159/159 - 26s - loss: 0.2388 - acc: 0.9245 - val_loss: 2.5646 - val_acc: 0.5636 - 26s/epoch - 164ms/step\n",
            "Epoch 79/100\n",
            "159/159 - 24s - loss: 0.2397 - acc: 0.9249 - val_loss: 2.5832 - val_acc: 0.5565 - 24s/epoch - 153ms/step\n",
            "Epoch 80/100\n",
            "159/159 - 24s - loss: 0.2472 - acc: 0.9223 - val_loss: 2.6173 - val_acc: 0.5518 - 24s/epoch - 152ms/step\n",
            "Epoch 81/100\n",
            "159/159 - 26s - loss: 0.2442 - acc: 0.9254 - val_loss: 2.6308 - val_acc: 0.5470 - 26s/epoch - 162ms/step\n",
            "Epoch 82/100\n",
            "159/159 - 23s - loss: 0.2276 - acc: 0.9245 - val_loss: 2.6426 - val_acc: 0.5526 - 23s/epoch - 143ms/step\n",
            "Epoch 83/100\n",
            "159/159 - 26s - loss: 0.2205 - acc: 0.9280 - val_loss: 2.6601 - val_acc: 0.5573 - 26s/epoch - 166ms/step\n",
            "Epoch 84/100\n",
            "159/159 - 26s - loss: 0.2245 - acc: 0.9274 - val_loss: 2.6599 - val_acc: 0.5565 - 26s/epoch - 161ms/step\n",
            "Epoch 85/100\n",
            "159/159 - 24s - loss: 0.2244 - acc: 0.9294 - val_loss: 2.6559 - val_acc: 0.5573 - 24s/epoch - 152ms/step\n",
            "Epoch 86/100\n",
            "159/159 - 26s - loss: 0.2214 - acc: 0.9274 - val_loss: 2.6483 - val_acc: 0.5565 - 26s/epoch - 166ms/step\n",
            "Epoch 87/100\n",
            "159/159 - 24s - loss: 0.2232 - acc: 0.9278 - val_loss: 2.6798 - val_acc: 0.5534 - 24s/epoch - 149ms/step\n",
            "Epoch 88/100\n",
            "159/159 - 27s - loss: 0.2141 - acc: 0.9342 - val_loss: 2.6804 - val_acc: 0.5549 - 27s/epoch - 171ms/step\n",
            "Epoch 89/100\n",
            "159/159 - 32s - loss: 0.2287 - acc: 0.9270 - val_loss: 2.7028 - val_acc: 0.5557 - 32s/epoch - 201ms/step\n",
            "Epoch 90/100\n",
            "159/159 - 24s - loss: 0.2119 - acc: 0.9324 - val_loss: 2.7056 - val_acc: 0.5549 - 24s/epoch - 152ms/step\n",
            "Epoch 91/100\n",
            "159/159 - 23s - loss: 0.2168 - acc: 0.9314 - val_loss: 2.7477 - val_acc: 0.5462 - 23s/epoch - 144ms/step\n",
            "Epoch 92/100\n",
            "159/159 - 25s - loss: 0.2253 - acc: 0.9290 - val_loss: 2.7508 - val_acc: 0.5542 - 25s/epoch - 154ms/step\n",
            "Epoch 93/100\n",
            "159/159 - 24s - loss: 0.2119 - acc: 0.9328 - val_loss: 2.7530 - val_acc: 0.5439 - 24s/epoch - 152ms/step\n",
            "Epoch 94/100\n",
            "159/159 - 27s - loss: 0.2022 - acc: 0.9387 - val_loss: 2.7318 - val_acc: 0.5549 - 27s/epoch - 167ms/step\n",
            "Epoch 95/100\n",
            "159/159 - 24s - loss: 0.1980 - acc: 0.9355 - val_loss: 2.7690 - val_acc: 0.5502 - 24s/epoch - 152ms/step\n",
            "Epoch 96/100\n",
            "159/159 - 26s - loss: 0.2077 - acc: 0.9336 - val_loss: 2.7890 - val_acc: 0.5581 - 26s/epoch - 164ms/step\n",
            "Epoch 97/100\n",
            "159/159 - 26s - loss: 0.2029 - acc: 0.9342 - val_loss: 2.7915 - val_acc: 0.5581 - 26s/epoch - 163ms/step\n",
            "Epoch 98/100\n",
            "159/159 - 23s - loss: 0.2076 - acc: 0.9340 - val_loss: 2.8131 - val_acc: 0.5557 - 23s/epoch - 143ms/step\n",
            "Epoch 99/100\n",
            "159/159 - 26s - loss: 0.1970 - acc: 0.9357 - val_loss: 2.8137 - val_acc: 0.5510 - 26s/epoch - 166ms/step\n",
            "Epoch 100/100\n",
            "159/159 - 23s - loss: 0.2020 - acc: 0.9361 - val_loss: 2.8593 - val_acc: 0.5462 - 23s/epoch - 145ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b9bb0c11150>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a sequence of characters with the LLM\n",
        "# The LLM will attempt to predict the next 100 characters after taking bla bla\n",
        "\n",
        "def generator(model, mapping, seq_length, seed_text, n_chars):\n",
        "\n",
        "\tinput_text = seed_text\n",
        "\n",
        "\t# Generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\n",
        "\t\t# Encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in input_text]\n",
        "\n",
        "\t\t# Truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen = seq_length, truncating = \"pre\")\n",
        "\n",
        "\t\t# Predict character\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis = -1)\n",
        "\n",
        "\t\t# Reverse map integer to character\n",
        "\t\tout_char = \"\"\n",
        "\n",
        "\t\tfor char, index in mapping.items():\n",
        "\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# Append\n",
        "\t\tinput_text += char\n",
        "\n",
        "\treturn input_text\n",
        "\n",
        "# Calling the generator\n",
        "input = \"LLM is making new sentences \"\n",
        "\n",
        "print(len(input))\n",
        "print(generator(model, mapping, 40, input.lower(), 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5yLmwy6vyP-",
        "outputId": "62a847cc-a3ca-4379-da00-42d2bff5b297"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "llm is making new sentences that defing not only look and feel good but comporumetain and taking part healthy and fit lifestyle \n"
          ]
        }
      ]
    }
  ]
}