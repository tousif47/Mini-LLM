{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UfUxbsaaCK3JN5TuL5rJgifxrD9VvwIQ",
      "authorship_tag": "ABX9TyOACR9N5s7OaGgKGuU64lnN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tousif47/Mini-LLM/blob/main/Scratch_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ch-1Z_KoG9qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2ead24-dd3d-4312-c694-d342d72bec32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "\n",
        "print(data_text.read(100), \"\\n\\nNumber of characters in text file :\", len(data_text.read()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mYOG98Sl0PC",
        "outputId": "3c70fc55-cbc5-44f1-9840-e774d34ba2d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being healthy and fit in simple terms means taking good care of the body. We should remember that a  \n",
            "\n",
            "Number of characters in text file : 7030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the text data\n",
        "# Basic text preprocessing because a simple dataset with minimal noise has been selected to simplify the code\n",
        "# To maintain uniformity all words has been converted to lower case and any words with length less than 3 has been removed\n",
        "# Punctuations are also removed\n",
        "\n",
        "def text_preprocess(text):\n",
        "\n",
        "    # Lower case text\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "\n",
        "    # Remove punctuations\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    long_words = []\n",
        "\n",
        "    # Remove short words\n",
        "    for i in newString.split():\n",
        "        if len(i) >= 3:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "# Calling the preprocessor\n",
        "data_text = open(\"/content/drive/MyDrive/Practice Playing with code/health.txt\", \"r\")\n",
        "data_preprocessed = text_preprocess(data_text.read())\n",
        "\n",
        "print(data_preprocessed[:100])"
      ],
      "metadata": {
        "id": "cyXO-fBsmn6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032acf9b-f9f2-46a3-edff-2daadbfe4145"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "being healthy and fit simple terms means taking good care the body should remember that healthy mind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences\n",
        "# Taking 40 characters as context, model will try to predict the next character\n",
        "\n",
        "def sequencer(text):\n",
        "    length = 40\n",
        "    sequences = list()\n",
        "\n",
        "    for i in range(length, len(text)):\n",
        "\n",
        "        # Select sequence of tokens\n",
        "        seq = text[i-length:i+1]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(seq)\n",
        "\n",
        "    print(\"Total Sequences: %d\" % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# Calling the sequencer\n",
        "sequences = sequencer(data_preprocessed)\n",
        "print(\"\\n\", sequences[0], \"\\n\", sequences[1], \"\\n\", sequences[2], \"\\n\", sequences[3], \"\\n\", sequences[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T39XEfSXmT0r",
        "outputId": "41d131e6-b13a-4c41-e9cc-32add3c0f749"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 6322\n",
            "\n",
            " being healthy and fit simple terms means  \n",
            " eing healthy and fit simple terms means t \n",
            " ing healthy and fit simple terms means ta \n",
            " ng healthy and fit simple terms means tak \n",
            " g healthy and fit simple terms means taki\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the sequences\n",
        "\n",
        "chars = sorted(list(set(data_preprocessed)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encoder(seq):\n",
        "\n",
        "    sequences = list()\n",
        "\n",
        "    for line in seq:\n",
        "\n",
        "        # Integer encode line\n",
        "        encoded = [mapping[char] for char in line]\n",
        "\n",
        "        # Store\n",
        "        sequences.append(encoded)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "# Calling the encoder\n",
        "encoded_seq = encoder(sequences)\n",
        "print(encoded_seq[0], \"\\n\", encoded_seq[1], \"\\n\", encoded_seq[2], \"\\n\", encoded_seq[3], \"\\n\", encoded_seq[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLyzWf3Nqs4N",
        "outputId": "6aa41498-6618-45b3-9b0f-4a8e65bc6040"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0] \n",
            " [5, 9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20] \n",
            " [9, 14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1] \n",
            " [14, 7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11] \n",
            " [7, 0, 8, 5, 1, 12, 20, 8, 25, 0, 1, 14, 4, 0, 6, 9, 20, 0, 19, 9, 13, 16, 12, 5, 0, 20, 5, 18, 13, 19, 0, 13, 5, 1, 14, 19, 0, 20, 1, 11, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating training and testing split from the dataset\n",
        "\n",
        "# Vocabulary size\n",
        "vocabulary = len(mapping)\n",
        "encoded_seq = np.array(encoded_seq)\n",
        "\n",
        "X, y = encoded_seq[:,:-1], encoded_seq[:,-1]\n",
        "\n",
        "# One hot encode y\n",
        "y = to_categorical(y, num_classes = vocabulary)\n",
        "\n",
        "# Training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 47)\n",
        "\n",
        "print(\"Train set:\", X_train.shape, \"Test set:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNmXZjRYtdIA",
        "outputId": "15c44f81-938d-41b4-f90e-0fd86e5c41ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: (5689, 40) Test set: (633, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the LLM\n",
        "# Embedding layer of Keras is used to creat a 50 dimension embedding for each character\n",
        "# GRU layer is used as base model with 150 timestamps\n",
        "# A Dense layer is used with a softmax activation function for prediction\n",
        "\n",
        "# Defining\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary, 50, input_length = 40, trainable = True))\n",
        "model.add(GRU(150, recurrent_dropout = 0.1, dropout = 0.1))\n",
        "model.add(Dense(vocabulary, activation = \"softmax\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Compiling\n",
        "model.compile(loss = \"categorical_crossentropy\", metrics = [\"acc\"], optimizer=\"adam\")\n",
        "\n",
        "# Fitting\n",
        "model.fit(X_train, y_train, epochs = 100, verbose = 2, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4i-ebMMuRPj",
        "outputId": "aadd4806-3827-4aca-a547-6e6bfa477019"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 50)            1350      \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 150)               90900     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 27)                4077      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,327\n",
            "Trainable params: 96,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "178/178 - 29s - loss: 2.7969 - acc: 0.1918 - val_loss: 2.5465 - val_acc: 0.2449 - 29s/epoch - 166ms/step\n",
            "Epoch 2/100\n",
            "178/178 - 26s - loss: 2.2995 - acc: 0.3252 - val_loss: 2.2704 - val_acc: 0.3207 - 26s/epoch - 146ms/step\n",
            "Epoch 3/100\n",
            "178/178 - 25s - loss: 2.1274 - acc: 0.3765 - val_loss: 2.1453 - val_acc: 0.3381 - 25s/epoch - 139ms/step\n",
            "Epoch 4/100\n",
            "178/178 - 25s - loss: 1.9997 - acc: 0.4048 - val_loss: 2.0386 - val_acc: 0.3681 - 25s/epoch - 142ms/step\n",
            "Epoch 5/100\n",
            "178/178 - 26s - loss: 1.8871 - acc: 0.4322 - val_loss: 1.9467 - val_acc: 0.4139 - 26s/epoch - 149ms/step\n",
            "Epoch 6/100\n",
            "178/178 - 24s - loss: 1.7782 - acc: 0.4649 - val_loss: 1.8830 - val_acc: 0.4281 - 24s/epoch - 132ms/step\n",
            "Epoch 7/100\n",
            "178/178 - 26s - loss: 1.6802 - acc: 0.4952 - val_loss: 1.7898 - val_acc: 0.4597 - 26s/epoch - 148ms/step\n",
            "Epoch 8/100\n",
            "178/178 - 26s - loss: 1.5751 - acc: 0.5293 - val_loss: 1.7364 - val_acc: 0.4961 - 26s/epoch - 145ms/step\n",
            "Epoch 9/100\n",
            "178/178 - 24s - loss: 1.4923 - acc: 0.5491 - val_loss: 1.6840 - val_acc: 0.5182 - 24s/epoch - 133ms/step\n",
            "Epoch 10/100\n",
            "178/178 - 31s - loss: 1.4164 - acc: 0.5732 - val_loss: 1.6384 - val_acc: 0.5276 - 31s/epoch - 174ms/step\n",
            "Epoch 11/100\n",
            "178/178 - 26s - loss: 1.3319 - acc: 0.5954 - val_loss: 1.6242 - val_acc: 0.5466 - 26s/epoch - 149ms/step\n",
            "Epoch 12/100\n",
            "178/178 - 23s - loss: 1.2672 - acc: 0.6117 - val_loss: 1.6026 - val_acc: 0.5482 - 23s/epoch - 132ms/step\n",
            "Epoch 13/100\n",
            "178/178 - 26s - loss: 1.1992 - acc: 0.6312 - val_loss: 1.5774 - val_acc: 0.5545 - 26s/epoch - 147ms/step\n",
            "Epoch 14/100\n",
            "178/178 - 27s - loss: 1.1376 - acc: 0.6516 - val_loss: 1.5764 - val_acc: 0.5671 - 27s/epoch - 153ms/step\n",
            "Epoch 15/100\n",
            "178/178 - 24s - loss: 1.0775 - acc: 0.6676 - val_loss: 1.5548 - val_acc: 0.5719 - 24s/epoch - 134ms/step\n",
            "Epoch 16/100\n",
            "178/178 - 26s - loss: 1.0266 - acc: 0.6785 - val_loss: 1.5671 - val_acc: 0.5893 - 26s/epoch - 149ms/step\n",
            "Epoch 17/100\n",
            "178/178 - 25s - loss: 0.9775 - acc: 0.6941 - val_loss: 1.5615 - val_acc: 0.5877 - 25s/epoch - 138ms/step\n",
            "Epoch 18/100\n",
            "178/178 - 25s - loss: 0.9415 - acc: 0.7043 - val_loss: 1.5963 - val_acc: 0.5624 - 25s/epoch - 138ms/step\n",
            "Epoch 19/100\n",
            "178/178 - 27s - loss: 0.8939 - acc: 0.7207 - val_loss: 1.5925 - val_acc: 0.5829 - 27s/epoch - 149ms/step\n",
            "Epoch 20/100\n",
            "178/178 - 24s - loss: 0.8595 - acc: 0.7272 - val_loss: 1.6138 - val_acc: 0.5766 - 24s/epoch - 132ms/step\n",
            "Epoch 21/100\n",
            "178/178 - 26s - loss: 0.8224 - acc: 0.7423 - val_loss: 1.6152 - val_acc: 0.5766 - 26s/epoch - 148ms/step\n",
            "Epoch 22/100\n",
            "178/178 - 25s - loss: 0.7774 - acc: 0.7529 - val_loss: 1.6388 - val_acc: 0.5956 - 25s/epoch - 140ms/step\n",
            "Epoch 23/100\n",
            "178/178 - 24s - loss: 0.7444 - acc: 0.7625 - val_loss: 1.6455 - val_acc: 0.5861 - 24s/epoch - 137ms/step\n",
            "Epoch 24/100\n",
            "178/178 - 26s - loss: 0.7247 - acc: 0.7681 - val_loss: 1.6468 - val_acc: 0.6019 - 26s/epoch - 148ms/step\n",
            "Epoch 25/100\n",
            "178/178 - 24s - loss: 0.6887 - acc: 0.7838 - val_loss: 1.6583 - val_acc: 0.6003 - 24s/epoch - 132ms/step\n",
            "Epoch 26/100\n",
            "178/178 - 26s - loss: 0.6640 - acc: 0.7877 - val_loss: 1.6952 - val_acc: 0.5956 - 26s/epoch - 148ms/step\n",
            "Epoch 27/100\n",
            "178/178 - 27s - loss: 0.6360 - acc: 0.7975 - val_loss: 1.7040 - val_acc: 0.6003 - 27s/epoch - 150ms/step\n",
            "Epoch 28/100\n",
            "178/178 - 23s - loss: 0.6113 - acc: 0.8096 - val_loss: 1.7229 - val_acc: 0.6082 - 23s/epoch - 132ms/step\n",
            "Epoch 29/100\n",
            "178/178 - 27s - loss: 0.6023 - acc: 0.8119 - val_loss: 1.7633 - val_acc: 0.6003 - 27s/epoch - 150ms/step\n",
            "Epoch 30/100\n",
            "178/178 - 26s - loss: 0.5891 - acc: 0.8131 - val_loss: 1.7508 - val_acc: 0.5893 - 26s/epoch - 147ms/step\n",
            "Epoch 31/100\n",
            "178/178 - 24s - loss: 0.5593 - acc: 0.8247 - val_loss: 1.7819 - val_acc: 0.5924 - 24s/epoch - 136ms/step\n",
            "Epoch 32/100\n",
            "178/178 - 26s - loss: 0.5334 - acc: 0.8371 - val_loss: 1.7750 - val_acc: 0.6066 - 26s/epoch - 148ms/step\n",
            "Epoch 33/100\n",
            "178/178 - 25s - loss: 0.5172 - acc: 0.8365 - val_loss: 1.8161 - val_acc: 0.5956 - 25s/epoch - 138ms/step\n",
            "Epoch 34/100\n",
            "178/178 - 25s - loss: 0.5067 - acc: 0.8439 - val_loss: 1.8387 - val_acc: 0.5972 - 25s/epoch - 143ms/step\n",
            "Epoch 35/100\n",
            "178/178 - 26s - loss: 0.4845 - acc: 0.8483 - val_loss: 1.8482 - val_acc: 0.5908 - 26s/epoch - 148ms/step\n",
            "Epoch 36/100\n",
            "178/178 - 24s - loss: 0.4816 - acc: 0.8467 - val_loss: 1.8857 - val_acc: 0.5940 - 24s/epoch - 133ms/step\n",
            "Epoch 37/100\n",
            "178/178 - 26s - loss: 0.4612 - acc: 0.8504 - val_loss: 1.8845 - val_acc: 0.5987 - 26s/epoch - 147ms/step\n",
            "Epoch 38/100\n",
            "178/178 - 25s - loss: 0.4490 - acc: 0.8617 - val_loss: 1.9461 - val_acc: 0.5861 - 25s/epoch - 142ms/step\n",
            "Epoch 39/100\n",
            "178/178 - 25s - loss: 0.4293 - acc: 0.8689 - val_loss: 1.9394 - val_acc: 0.5845 - 25s/epoch - 140ms/step\n",
            "Epoch 40/100\n",
            "178/178 - 27s - loss: 0.4302 - acc: 0.8648 - val_loss: 1.9580 - val_acc: 0.5845 - 27s/epoch - 149ms/step\n",
            "Epoch 41/100\n",
            "178/178 - 24s - loss: 0.4177 - acc: 0.8696 - val_loss: 1.9717 - val_acc: 0.5782 - 24s/epoch - 134ms/step\n",
            "Epoch 42/100\n",
            "178/178 - 26s - loss: 0.4040 - acc: 0.8706 - val_loss: 2.0056 - val_acc: 0.5845 - 26s/epoch - 145ms/step\n",
            "Epoch 43/100\n",
            "178/178 - 27s - loss: 0.4048 - acc: 0.8724 - val_loss: 2.0220 - val_acc: 0.5908 - 27s/epoch - 152ms/step\n",
            "Epoch 44/100\n",
            "178/178 - 24s - loss: 0.3857 - acc: 0.8792 - val_loss: 2.0145 - val_acc: 0.5798 - 24s/epoch - 134ms/step\n",
            "Epoch 45/100\n",
            "178/178 - 27s - loss: 0.3763 - acc: 0.8815 - val_loss: 2.0461 - val_acc: 0.5877 - 27s/epoch - 150ms/step\n",
            "Epoch 46/100\n",
            "178/178 - 25s - loss: 0.3619 - acc: 0.8917 - val_loss: 2.0354 - val_acc: 0.5829 - 25s/epoch - 142ms/step\n",
            "Epoch 47/100\n",
            "178/178 - 24s - loss: 0.3640 - acc: 0.8849 - val_loss: 2.0755 - val_acc: 0.5814 - 24s/epoch - 137ms/step\n",
            "Epoch 48/100\n",
            "178/178 - 26s - loss: 0.3531 - acc: 0.8877 - val_loss: 2.0837 - val_acc: 0.5924 - 26s/epoch - 149ms/step\n",
            "Epoch 49/100\n",
            "178/178 - 23s - loss: 0.3412 - acc: 0.8931 - val_loss: 2.1150 - val_acc: 0.5814 - 23s/epoch - 131ms/step\n",
            "Epoch 50/100\n",
            "178/178 - 27s - loss: 0.3519 - acc: 0.8880 - val_loss: 2.1114 - val_acc: 0.5845 - 27s/epoch - 149ms/step\n",
            "Epoch 51/100\n",
            "178/178 - 26s - loss: 0.3377 - acc: 0.8930 - val_loss: 2.1328 - val_acc: 0.5908 - 26s/epoch - 147ms/step\n",
            "Epoch 52/100\n",
            "178/178 - 24s - loss: 0.3445 - acc: 0.8880 - val_loss: 2.1972 - val_acc: 0.5735 - 24s/epoch - 133ms/step\n",
            "Epoch 53/100\n",
            "178/178 - 26s - loss: 0.3303 - acc: 0.8966 - val_loss: 2.1804 - val_acc: 0.5814 - 26s/epoch - 148ms/step\n",
            "Epoch 54/100\n",
            "178/178 - 24s - loss: 0.3202 - acc: 0.9003 - val_loss: 2.2260 - val_acc: 0.5861 - 24s/epoch - 135ms/step\n",
            "Epoch 55/100\n",
            "178/178 - 25s - loss: 0.3187 - acc: 0.8970 - val_loss: 2.2085 - val_acc: 0.5766 - 25s/epoch - 142ms/step\n",
            "Epoch 56/100\n",
            "178/178 - 26s - loss: 0.3118 - acc: 0.9019 - val_loss: 2.2461 - val_acc: 0.5845 - 26s/epoch - 148ms/step\n",
            "Epoch 57/100\n",
            "178/178 - 23s - loss: 0.3105 - acc: 0.9016 - val_loss: 2.2644 - val_acc: 0.5735 - 23s/epoch - 130ms/step\n",
            "Epoch 58/100\n",
            "178/178 - 26s - loss: 0.3029 - acc: 0.9054 - val_loss: 2.2634 - val_acc: 0.5845 - 26s/epoch - 148ms/step\n",
            "Epoch 59/100\n",
            "178/178 - 25s - loss: 0.2864 - acc: 0.9095 - val_loss: 2.2438 - val_acc: 0.5845 - 25s/epoch - 142ms/step\n",
            "Epoch 60/100\n",
            "178/178 - 24s - loss: 0.2849 - acc: 0.9097 - val_loss: 2.3098 - val_acc: 0.5798 - 24s/epoch - 136ms/step\n",
            "Epoch 61/100\n",
            "178/178 - 26s - loss: 0.3042 - acc: 0.9046 - val_loss: 2.3294 - val_acc: 0.5750 - 26s/epoch - 147ms/step\n",
            "Epoch 62/100\n",
            "178/178 - 24s - loss: 0.2999 - acc: 0.9061 - val_loss: 2.3320 - val_acc: 0.5782 - 24s/epoch - 132ms/step\n",
            "Epoch 63/100\n",
            "178/178 - 27s - loss: 0.2824 - acc: 0.9091 - val_loss: 2.3319 - val_acc: 0.5750 - 27s/epoch - 149ms/step\n",
            "Epoch 64/100\n",
            "178/178 - 26s - loss: 0.2839 - acc: 0.9109 - val_loss: 2.3871 - val_acc: 0.5829 - 26s/epoch - 144ms/step\n",
            "Epoch 65/100\n",
            "178/178 - 24s - loss: 0.2703 - acc: 0.9149 - val_loss: 2.3734 - val_acc: 0.5766 - 24s/epoch - 133ms/step\n",
            "Epoch 66/100\n",
            "178/178 - 26s - loss: 0.2702 - acc: 0.9133 - val_loss: 2.4298 - val_acc: 0.5703 - 26s/epoch - 147ms/step\n",
            "Epoch 67/100\n",
            "178/178 - 24s - loss: 0.2681 - acc: 0.9176 - val_loss: 2.4372 - val_acc: 0.5608 - 24s/epoch - 133ms/step\n",
            "Epoch 68/100\n",
            "178/178 - 26s - loss: 0.2712 - acc: 0.9151 - val_loss: 2.4298 - val_acc: 0.5687 - 26s/epoch - 145ms/step\n",
            "Epoch 69/100\n",
            "178/178 - 26s - loss: 0.2637 - acc: 0.9144 - val_loss: 2.4651 - val_acc: 0.5671 - 26s/epoch - 147ms/step\n",
            "Epoch 70/100\n",
            "178/178 - 23s - loss: 0.2696 - acc: 0.9142 - val_loss: 2.4495 - val_acc: 0.5687 - 23s/epoch - 132ms/step\n",
            "Epoch 71/100\n",
            "178/178 - 26s - loss: 0.2634 - acc: 0.9147 - val_loss: 2.4688 - val_acc: 0.5624 - 26s/epoch - 148ms/step\n",
            "Epoch 72/100\n",
            "178/178 - 24s - loss: 0.2602 - acc: 0.9169 - val_loss: 2.5459 - val_acc: 0.5703 - 24s/epoch - 136ms/step\n",
            "Epoch 73/100\n",
            "178/178 - 25s - loss: 0.2471 - acc: 0.9198 - val_loss: 2.4655 - val_acc: 0.5719 - 25s/epoch - 139ms/step\n",
            "Epoch 74/100\n",
            "178/178 - 26s - loss: 0.2576 - acc: 0.9190 - val_loss: 2.4885 - val_acc: 0.5766 - 26s/epoch - 148ms/step\n",
            "Epoch 75/100\n",
            "178/178 - 24s - loss: 0.2465 - acc: 0.9209 - val_loss: 2.5312 - val_acc: 0.5592 - 24s/epoch - 133ms/step\n",
            "Epoch 76/100\n",
            "178/178 - 26s - loss: 0.2434 - acc: 0.9204 - val_loss: 2.5524 - val_acc: 0.5687 - 26s/epoch - 148ms/step\n",
            "Epoch 77/100\n",
            "178/178 - 26s - loss: 0.2546 - acc: 0.9218 - val_loss: 2.5887 - val_acc: 0.5750 - 26s/epoch - 145ms/step\n",
            "Epoch 78/100\n",
            "178/178 - 24s - loss: 0.2468 - acc: 0.9186 - val_loss: 2.5823 - val_acc: 0.5687 - 24s/epoch - 133ms/step\n",
            "Epoch 79/100\n",
            "178/178 - 26s - loss: 0.2490 - acc: 0.9190 - val_loss: 2.5885 - val_acc: 0.5687 - 26s/epoch - 148ms/step\n",
            "Epoch 80/100\n",
            "178/178 - 24s - loss: 0.2524 - acc: 0.9188 - val_loss: 2.6212 - val_acc: 0.5687 - 24s/epoch - 133ms/step\n",
            "Epoch 81/100\n",
            "178/178 - 26s - loss: 0.2425 - acc: 0.9242 - val_loss: 2.5977 - val_acc: 0.5735 - 26s/epoch - 144ms/step\n",
            "Epoch 82/100\n",
            "178/178 - 26s - loss: 0.2528 - acc: 0.9167 - val_loss: 2.6313 - val_acc: 0.5750 - 26s/epoch - 148ms/step\n",
            "Epoch 83/100\n",
            "178/178 - 24s - loss: 0.2307 - acc: 0.9227 - val_loss: 2.6398 - val_acc: 0.5782 - 24s/epoch - 133ms/step\n",
            "Epoch 84/100\n",
            "178/178 - 26s - loss: 0.2228 - acc: 0.9274 - val_loss: 2.6623 - val_acc: 0.5624 - 26s/epoch - 148ms/step\n",
            "Epoch 85/100\n",
            "178/178 - 25s - loss: 0.2284 - acc: 0.9246 - val_loss: 2.6608 - val_acc: 0.5640 - 25s/epoch - 141ms/step\n",
            "Epoch 86/100\n",
            "178/178 - 25s - loss: 0.2330 - acc: 0.9232 - val_loss: 2.6352 - val_acc: 0.5656 - 25s/epoch - 139ms/step\n",
            "Epoch 87/100\n",
            "178/178 - 26s - loss: 0.2387 - acc: 0.9216 - val_loss: 2.6921 - val_acc: 0.5624 - 26s/epoch - 148ms/step\n",
            "Epoch 88/100\n",
            "178/178 - 23s - loss: 0.2350 - acc: 0.9234 - val_loss: 2.6785 - val_acc: 0.5608 - 23s/epoch - 131ms/step\n",
            "Epoch 89/100\n",
            "178/178 - 26s - loss: 0.2197 - acc: 0.9276 - val_loss: 2.6787 - val_acc: 0.5687 - 26s/epoch - 147ms/step\n",
            "Epoch 90/100\n",
            "178/178 - 26s - loss: 0.2358 - acc: 0.9183 - val_loss: 2.7002 - val_acc: 0.5592 - 26s/epoch - 148ms/step\n",
            "Epoch 91/100\n",
            "178/178 - 24s - loss: 0.2257 - acc: 0.9230 - val_loss: 2.6885 - val_acc: 0.5687 - 24s/epoch - 133ms/step\n",
            "Epoch 92/100\n",
            "178/178 - 26s - loss: 0.2129 - acc: 0.9320 - val_loss: 2.7098 - val_acc: 0.5561 - 26s/epoch - 146ms/step\n",
            "Epoch 93/100\n",
            "178/178 - 24s - loss: 0.2138 - acc: 0.9341 - val_loss: 2.7094 - val_acc: 0.5577 - 24s/epoch - 138ms/step\n",
            "Epoch 94/100\n",
            "178/178 - 24s - loss: 0.2242 - acc: 0.9249 - val_loss: 2.7334 - val_acc: 0.5640 - 24s/epoch - 138ms/step\n",
            "Epoch 95/100\n",
            "178/178 - 26s - loss: 0.2212 - acc: 0.9262 - val_loss: 2.7374 - val_acc: 0.5671 - 26s/epoch - 148ms/step\n",
            "Epoch 96/100\n",
            "178/178 - 23s - loss: 0.2092 - acc: 0.9304 - val_loss: 2.7310 - val_acc: 0.5703 - 23s/epoch - 131ms/step\n",
            "Epoch 97/100\n",
            "178/178 - 26s - loss: 0.2074 - acc: 0.9321 - val_loss: 2.7461 - val_acc: 0.5671 - 26s/epoch - 148ms/step\n",
            "Epoch 98/100\n",
            "178/178 - 26s - loss: 0.2204 - acc: 0.9286 - val_loss: 2.7523 - val_acc: 0.5703 - 26s/epoch - 148ms/step\n",
            "Epoch 99/100\n",
            "178/178 - 24s - loss: 0.2147 - acc: 0.9272 - val_loss: 2.7802 - val_acc: 0.5687 - 24s/epoch - 133ms/step\n",
            "Epoch 100/100\n",
            "178/178 - 26s - loss: 0.2111 - acc: 0.9316 - val_loss: 2.7649 - val_acc: 0.5782 - 26s/epoch - 148ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78a8178aaf80>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a sequence of characters with the LLM\n",
        "# The LLM will attempt to predict the next 50 characters after taking bla bla\n",
        "\n",
        "def generator(model, mapping, seq_length, seed_text, n_chars):\n",
        "\n",
        "\tinput_text = seed_text\n",
        "\n",
        "\t# Generate a fixed number of characters\n",
        "\tfor _ in range(n_chars):\n",
        "\n",
        "\t\t# Encode the characters as integers\n",
        "\t\tencoded = [mapping[char] for char in input_text]\n",
        "\n",
        "\t\t# Truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen = seq_length, truncating = \"pre\")\n",
        "\n",
        "\t\t# Predict character\n",
        "\t\tyhat = np.argmax(model.predict(encoded), axis = -1)\n",
        "\n",
        "\t\t# Reverse map integer to character\n",
        "\t\tout_char = \"\"\n",
        "\n",
        "\t\tfor char, index in mapping.items():\n",
        "\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_char = char\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# Append\n",
        "\t\tinput_text += char\n",
        "\n",
        "\treturn input_text\n",
        "\n",
        "# Calling the generator\n",
        "input = \"Today is a very beautiful day\"\n",
        "\n",
        "print(len(input))\n",
        "print(generator(model, mapping, 30, input.lower(), 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5yLmwy6vyP-",
        "outputId": "f71a84d5-f7c3-43b7-c770-28e1e37c32b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "1/1 [==============================] - 1s 660ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "today is a very beautiful day should skipped food\n"
          ]
        }
      ]
    }
  ]
}